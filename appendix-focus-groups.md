# Appendix: Focus Group Methodology and Materials

This appendix provides detailed information about the focus group methodology, materials, and scenarios used in our study. The focus groups were designed to identify and prioritize stakeholder values for recommender systems in digital archives, specifically for Monasterium.net.


## Methodology Overview

We conducted structured focus groups with five stakeholder groups (upstream, provider, system, consumer, and downstream) to identify values for recommender systems in digital archives. Each group included five domain experts, partly representing high-profile institutions connected to Monasterium.net. Sessions lasted 60 minutes.


## Stakeholder Groups

| Group | Role |
|-------|------|
| Upstream (U1-U5) | Archivist, Curator, Librarian |
| Provider (P1-P5) | Manager, Specialist, Researcher |
| System (S1-S5) | Developer, Director, Specialist |
| Consumer (C1-C5) | Researcher, Educator, Student |
| Downstream (D1-D5) | Publisher, Editor, Technologist |


## Session Structure

Each 60-minute session followed this structure:
- Introduction and context-setting (10 minutes)
- Scenario 1: Content Visibility and Representation (15 minutes)
- Scenario 2: Expertise Adaptation and Access (15 minutes)
- Scenario 3: Transparency and Trust (15 minutes)
- Closing remarks and next steps (5 minutes)

Value prioritization and discussion were conducted freely within each scenario, allowing participants to explore and debate the issues most relevant to their stakeholder perspective.


## Discussion Materials

All scenarios were supported by visual materials, including:
- Mockups showing different recommendation approaches
- Visual representations of the challenges and trade-offs
- Real-world examples

The same core questions were used across all stakeholder groups for each scenario to ensure consistent discussion points while allowing for stakeholder-specific perspectives to emerge naturally. Provocative statements differed slightly since they were tailored to better match the stakeholder role.

### Scenario 1: Content Visibility and Representation

**Problem Statement:** Digital archives like Monasterium.net contain diverse collections from many institutions. Recommender systems must balance showing popular, well-documented items versus surfacing rare, potentially valuable but less-known items.

**Key Discussion Points:**
- Balancing visibility of popular vs. rare items
- Fair representation of different archives and collections
- Measuring scholarly merit

**Example Provocative Statement (Upstream):**
- "I'm more concerned that my highest-quality items receive visibility than ensuring equal representation of all my collection materials."

### Scenario 2: Expertise Adaptation and Access

**Problem Statement:** Users of Monasterium.net range from expert historians to students and the general public. Recommender systems must adapt to different levels of expertise and research needs.

**Key Discussion Points:**
- Adapting recommendations to user expertise levels
- Balancing scholarly depth with accessibility
- Supporting different research approaches

**Example Provocative Statement (Consumer):**
- "I prefer recommendations that confirm my existing research direction rather than those that challenge my assumptions."

### Scenario 3: Transparency and Trust

**Problem Statement:** Users need to understand and trust the recommendation process, particularly in scholarly contexts where the basis for recommendations affects research credibility.

**Key Discussion Points:**
- Building trust in recommendation quality
- Balancing transparency with system complexity
- Maintaining scholarly integrity

**Example Provocative Statement (Downstream):**
- "The scholarly integrity of publications based on recommended materials doesn't depend on understanding how those recommendations were generated."


## Analysis Approach

We analyzed transcripts using an abductive coding approach, combining deductive coding (using pre-established value categories) with inductive analysis to identify emerging patterns. The deductive framework incorporated five value categories (following a literature review and synthesis):
- Functional (relevance, diversity, novelty, serendipity, context awareness etc.)
- User experience (satisfaction, engagement, convenience, learning, curiosity etc. )
- Responsibility (fairness, transparency, trust, privacy, objectivity etc.)
- Human/social (equity, dignity, autonomy, well-being, interpretive freedom etc.)
- Technical (personalization, efficiency, scalability, maintainability, responsiveness etc.)

Initial analysis revealed recurring references to research and interaction stages, prompting a secondary coding round examining how values related to these stages. Cross-stakeholder comparative analysis identified patterns across groups, and values naturally aligned with four sequential stages we termed the "research funnel": discovery, interaction, integration, and impact.
